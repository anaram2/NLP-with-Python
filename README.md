# NLP with Python
Here you will find all the trainig tests I did during my NLP course with Python (includes: POS, regex, lemmatization, tokenization, deep-learning, etc.)


#Python-text-basics file: 
    · printing simple f-strings
    · creating a text file with a contact information
    · .read() and .close()
    · working with PDF files and PyPDF2
    · txt files and append mode
    · removing items from the txt file
    · using regex to extract email addresses from a pdf file


#NLP-basicsfile:
    · loading SpaCy and the English language library
    · using with open(), nlp() and .read()
    · counting the tokens in a file with len()
    · counting the number of sentences in the file
    · printing a certain sentence from the text
    · printing the text, pos tag,dep tag and lemma of the same sentence
    · create a matcher
    · print the text surrounding the matcher
    
#POS file:
    · For every token in the third sentence, print the token text, the POS tag, the fine-grained TAG tag, and the description of the fine-grained tag.
    · Provide a frequency list of POS tags from the entire document
    · What percentage of tokens are nouns?
    · Display the Dependency Parse for the third sentence
    · Show the first two named entities from Beatrix Potter's The Tale of Peter Rabbit
    · How many sentences are contained in The Tale of Peter Rabbit?


#Text-classification file:
    · import numpy, sklearn and pandas
    · Check for missing values in the data
    · Remove NaN values
    · Take a quick look at the `label` column and Split the data into train & test sets
    · Build a pipeline to vectorize the date, then train and fit a model.
    · Run predictions and analyze the results

#Sentiment-analysis file# Sentiment Analysis Assessment - Solution
    · Perform vector arithmetic on your own words
    · Perform VADER Sentiment Analysis on your own review

#LDA-NMF project Topic modeling file:
    · Use TF-IDF Vectorization to create a vectorized document term matrix.
    · Using Scikit-Learn create an instance of NMF with 20 expected components
    · Print our the top 15 most common words for each of the 20 topics.
    · Add a new column to the original quora dataframe that labels each question into one of the 20 topic categories.

#Keras-Basics file:
    · Deep learning with Keras and Tensorflow
    · Split the Data into Training and Test
    · Standardizing the Data
    · Building the Network with Keras
    · Fit (Train) the Model
    · Predicting New Unseen Data
    · Evaluating Model Performance
    · Saving and Loading Models

#Text generation with neural networks file. Functions for processing text:
    · Reading in files as a string text
    · Tokenize and Clean Text
    · Create Sequences of Tokens
    · Keras Tokenization
    · Convert to Numpy Matrix
    · Creating an LSTM based model
    · Train / Test Split
    · Training the Model
    · Generating New Text
    · Grab a random seed sequence
    · Exploring Generated Sequence

#Chat-bots file:
    · Loading the Data
    · Exploring the Format of the Data
    · Setting up Vocabulary of All Words
    · Vectorizing the Data
    · Functionalize Vectorization
    · Creating the Model: Placeholders for Inputs, Building the Networks
    · Encoders: Input Encoder m, Input Encoder c, Question Encoder, Encode the Sequences, Encode the Sequences, Use dot product to compute the match between first input vector seq and the query, Add this match matrix with the second input vector sequence, Concatenate, Saving the Model
    · Evaluating the Model: Plotting Out Training History, Evaluating on Given Test Set.
    · Writing Your Own Stories and Questions

#Building a corpus from individual files:
    · Using Python's `os` module to build a DataFram
    · Let's look at what os.walk() does
    · Use os.walk() to build a DataFrame

#Working with Stop Words
    · spaCy's built-in stopwords
    · NLTK's built-in stopwords
    · Scikit-learn's built-in stopwords


    
