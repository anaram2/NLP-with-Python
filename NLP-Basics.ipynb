{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a href='http://www.pieriandata.com'> <img src='../Pierian_Data_Logo.png' /></a>\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Basics Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assessment we'll be using the short story [_An Occurrence at Owl Creek Bridge_](https://en.wikipedia.org/wiki/An_Occurrence_at_Owl_Creek_Bridge) by Ambrose Bierce (1890). <br>The story is in the public domain; the text file was obtained from [Project Gutenberg](https://www.gutenberg.org/ebooks/375.txt.utf-8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL to perform standard imports:\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Create a Doc object from the file `owlcreek.txt`**<br>\n",
    "> HINT: Use `with open('../TextFiles/owlcreek.txt') as f:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here:\n",
    "with open('/Users/anaramirez/Documents/Cursos/Udemy/UPDATED_NLP_COURSE/TextFiles/owlcreek.txt', 'r') as f:\n",
    "    doc = nlp(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AN OCCURRENCE AT OWL CREEK BRIDGE\n",
       "\n",
       "by Ambrose Bierce\n",
       "\n",
       "I\n",
       "\n",
       "A man stood upon a railroad bridge in northern Alabama, looking down\n",
       "into the swift water twenty feet below.  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell to verify it worked:\n",
    "\n",
    "doc[:36]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. How many tokens are contained in the file?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4835"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. How many sentences are contained in the file?**<br>HINT: You'll want to build a list first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sentences= [] \n",
    "\n",
    "for sent in doc.sents:\n",
    "    my_sentences.append(sent)\n",
    "\n",
    "len(my_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Print the second sentence in the document**<br> HINT: Indexing starts at zero, and the title counts as the first sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The man's hands were behind\n",
       "his back, the wrists bound with a cord.  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sentences[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 5. For each token in the sentence above, print its `text`, `POS` tag, `dep` tag and `lemma`<br>\n",
    "CHALLENGE: Have values line up in columns in the print output.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DET 415 7425985699627899538\n",
      "man NOUN 440 3104811030673030468\n",
      "'s PART 8110129090154140942 16428057658620181782\n",
      "hands NOUN 429 10690717480206833971\n",
      "were AUX 8206900633647566924 10382539506755952630\n",
      "behind ADP 443 9368086581607646285\n",
      "\n",
      " SPACE 414 962983613142996970\n",
      "his PRON 440 2661093235354845946\n",
      "back NOUN 439 15255859468896132977\n",
      ", PUNCT 445 2593208677638477497\n",
      "the DET 415 7425985699627899538\n",
      "wrists NOUN 403 40049004327531306\n",
      "bound VERB 451 16578919470474021089\n",
      "with ADP 443 12510949447758279278\n",
      "a DET 415 11901859001352538922\n",
      "cord NOUN 439 8978229881582480158\n",
      ". PUNCT 445 12646065887601541794\n",
      "  SPACE 414 8532415787641010193\n"
     ]
    }
   ],
   "source": [
    "# NORMAL SOLUTION:\n",
    "\n",
    "for token in my_sentences[1]:\n",
    "    print(token.text, token.pos_, token.dep, token.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The \t DET \t 7425985699627899538 \t the\n",
      "man \t NOUN \t 3104811030673030468 \t man\n",
      "'s \t PART \t 16428057658620181782 \t 's\n",
      "hands \t NOUN \t 10690717480206833971 \t hand\n",
      "were \t AUX \t 10382539506755952630 \t be\n",
      "behind \t ADP \t 9368086581607646285 \t behind\n",
      "\n",
      " \t SPACE \t 962983613142996970 \t \n",
      "\n",
      "his \t PRON \t 2661093235354845946 \t his\n",
      "back \t NOUN \t 15255859468896132977 \t back\n",
      ", \t PUNCT \t 2593208677638477497 \t ,\n",
      "the \t DET \t 7425985699627899538 \t the\n",
      "wrists \t NOUN \t 40049004327531306 \t wrist\n",
      "bound \t VERB \t 16578919470474021089 \t bind\n",
      "with \t ADP \t 12510949447758279278 \t with\n",
      "a \t DET \t 11901859001352538922 \t a\n",
      "cord \t NOUN \t 8978229881582480158 \t cord\n",
      ". \t PUNCT \t 12646065887601541794 \t .\n",
      "  \t SPACE \t 8532415787641010193 \t  \n"
     ]
    }
   ],
   "source": [
    "# CHALLENGE SOLUTION:\n",
    "for token in my_sentences[1]:\n",
    "    print(token.text, '\\t', token.pos_, '\\t', token.lemma, '\\t', token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Write a matcher called 'Swimming' that finds both occurrences of the phrase \"swimming vigorously\" in the text**<br>\n",
    "HINT: You should include an `'IS_SPACE': True` pattern between the two words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Matcher library:\n",
    "\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pattern and add it to matcher:\n",
    "pattern1 = [{'LOWER': 'swimming'}, {'IS_SPACE': True}, {'LOWER': 'vigorously'}]\n",
    "\n",
    "matcher.add('swimming vigorously', [pattern1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(473010745751028843, 1274, 1277), (473010745751028843, 3609, 3612)]\n"
     ]
    }
   ],
   "source": [
    "# Create a list of matches called \"found_matches\" and print the list:\n",
    "found_matches = matcher(doc)\n",
    "\n",
    "print(found_matches)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Print the text surrounding each found match**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "By diving I could evade the bullets and, swimming\n",
       "vigorously, reach the bank, take to the woods and get away home."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[1265:1291]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "he was now swimming\n",
       "vigorously with the current."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[3606:3616]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXTRA CREDIT:<br>Print the *sentence* that contains each found match**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1265 1292\n",
      "3596 3617\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sents = [sent for sent in doc.sents]\n",
    "\n",
    "print(sents[51].start, sents[51].end)\n",
    "\n",
    "\n",
    "print(supermatches[149].start, supermatches[149].end)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'spacy.tokens.token.Token' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m doc:\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sent[\u001b[38;5;241m5\u001b[39m][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m doc\u001b[38;5;241m.\u001b[39mend:  \u001b[38;5;66;03m# this is the fifth match, that starts at doc3[673]\u001b[39;00m\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28mprint\u001b[39m(sent)\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'spacy.tokens.token.Token' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "#for sent in sents:\n",
    " #   if found_matchesmatches[4][1] < sent.end:  # this is the fifth match, that starts at doc3[673]\n",
    " #       print(sent)\n",
    "  #      break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great Job!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
